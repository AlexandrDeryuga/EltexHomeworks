#include <err.h>
#include <netinet/in.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/un.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <errno.h>
#include <fcntl.h>
#include <netinet/in.h>

#include <sys/epoll.h>
#include <sys/types.h>

#define STR_SIZE_MAX 255
#define PORT 8080
#define CLIENTS 2 // tcp и udp клиенты
#define MAX_EVENTS 2 


 struct sockaddr_in          peer_tcp,  my_tcp, peer_udp;
  int tcp_fd, new_tcp_fd, udp_fd; // new_sock_fd  это после акцепта, его должны использовать, см. активный и пассивный сокет
  
  
int CheckError(int err_n, char *err_str, int line) {          //Подсмотрел, понравилась конструкция
  if (err_n < 0) {   
   err(EXIT_FAILURE, "%s: %d", err_str, line);   // ну или как обычно perror можно использовать
   exit(1);
  } 
  return err_n;
}

void Fill_ip(struct sockaddr_in saddr, char *ip, int line)
{
strcpy(ip,"error");
if (inet_ntop(AF_INET,&saddr.sin_addr.s_addr, ip, 20) == NULL) {
              perror("inet_ntop\n");
              err(EXIT_FAILURE, "Fill_ip: %d\n", line);
              
           }
}

void Init()
{
socklen_t len = sizeof(struct sockaddr_in);  // это указатель на размер передаваемого сообщения, но размер может быть не тем который говорим, 
socklen_t *plen=&len;      //поэтому используется указатель, и он рестрикт, поэтому должен быть единственный, вот и объявляем его явно.
char ip[20];

  memset(&peer_tcp, 0, len);     // забьем всю структуру нулем
  memset(&my_tcp, 0, len);     // забьем всю структуру нулем
  memset(&peer_udp, 0, len);     // забьем всю структуру нулем
 
  
  // переведем IP строку в число


  peer_udp.sin_family = AF_INET;
  peer_udp.sin_addr.s_addr = inet_addr("127.0.0.1"); // можно и так
  peer_udp.sin_port = htons(PORT);

  
  CheckError(udp_fd = socket(AF_INET,SOCK_DGRAM, 0), "socket", __LINE__);  //  1.  (SOCK_DGRAM 0) - udp,  (SOCK_STREAM 0) - tcp
  CheckError(bind(udp_fd, (struct sockaddr *)&peer_udp, len),"bind", __LINE__); //1 - обьявили и забиндили
  
  Fill_ip(peer_udp, ip, __LINE__);
  
  printf("(epoll) NETWORK UDP Сервер запущен на сокете: %s %d\n",ip,ntohs(peer_udp.sin_port));
  
  
  
  peer_tcp.sin_family = AF_INET;
  peer_tcp.sin_addr.s_addr = inet_addr("127.0.0.1"); // можно и так
  peer_tcp.sin_port = htons(PORT);


  CheckError(tcp_fd = socket(AF_INET,SOCK_STREAM, 0), "socket", __LINE__);
  CheckError(bind(tcp_fd, (struct sockaddr *)&peer_tcp, len),"bind", __LINE__); //1 - обьявили и забиндили
  
  Fill_ip(peer_tcp, ip, __LINE__);
  
  CheckError(listen(tcp_fd, 5), "listen", __LINE__);                                     //  2. повесили слушать, и ушли в блок до ответа клиента
  printf("(epoll) NETWORK TCP Сервер запущен на сокете: %s %d\n",ip,ntohs(peer_tcp.sin_port));
   
  printf("TCP Сервер> ждем tcp клиента \n");
  CheckError(new_tcp_fd = accept( tcp_fd, (struct sockaddr *)&my_tcp, &len),"accept", __LINE__); 
  printf("TCP Сервер> соединение с tcp клиентом установленно, сервер готов к работе \n");


}

void set_nonblocking(int fd) {    //для мультеплекса надо установить не блокирующий режим, по умолчанию блокирующин функции wtite read
  int flags = fcntl(fd, F_GETFL, 0); //получить флаги
  if (flags == -1) {
    perror("fcntl()");
    return;
  }
  if (fcntl(fd, F_SETFL, flags | O_NONBLOCK) == -1) {  // добавить флаг
    perror("fcntl()");
  }
}

int main() {
 
  char msg_recv_send[STR_SIZE_MAX] = {0};
 

  socklen_t len = sizeof(struct sockaddr_in);  // это указатель на размер передаваемого сообщения, но размер может быть не тем который говорим, 
  socklen_t *plen=&len;      //поэтому используется указатель, и он рестрикт, поэтому должен быть единственный, вот и объявляем его явно.


Init();



// мультиплекс

//  select проще и медленнее, но работает везде! poll -тот же селект, обертка

/*Группа вызовов epoll является наиболее развитым мультиплексером в ядре Linux и способна работать в двух режимах:

    level-triggered - похожий на select упрощённый режим, в котором файловый дескриптор возвращается, если остались непрочитанные данные
        если приложение прочитало только часть доступных данных, вызов epoll вернёт ему недопрочитанные дескрипторы
    edge-triggered - файловый дескриптор с событием возвращается только если с момента последнего возврата epoll произошли новые события (например, пришли новые данные)
        если приложение прочитало только часть доступных данных, в данном режиме оно всё равно будет заблокировано до прихода каких-либо новых данных

Чтобы глубже понять происходящее, рассмотрим схему работы epoll с точки зрения ядра. Допустим, приложение с помощью epoll начало мониторинг поступления данных для чтения из какого-либо файла. Для этого приложение вызывает epoll_wait и засыпает на этом вызове. Ядро хранит связь между ожидающими данных потоками и файловым дескриптором, который один или несколько потоков (или процессов) отслеживают. В случае поступления порции данных ядро обходит список ожидающих потоков и разблокирует их, что для потока выглядит как возврат из функции epoll_wait.

    В случае level-triggered режима вызов epoll_wait пройдёт по списку файловых дескрипторов и проверит, не соблюдается ли в данный момент условие, которое интересует приложение, что может привести к возврату из epoll_wait без какой-либо блокировки.
    В случае edge-triggered режима ядро пропускает такую проверку и усыпляет поток, пока не обнаружит событие прихода данных на одном из файловых дескрипторов, за которыми следит поток. Такой режим превращает epoll в мультиплексер с алгоритмической сложностью O(1): после прихода новых данных из файла ядро сразу же знает, какой процесс надо пробудить.

Для использования edge-triggered режима нужно поместить файловые дескрипторы в неблокирующий режим, и на каждой итерации вызывать read либо write до тех пор, пока они не установят код ошибки EWOULDBLOCK
*/


/*
Прерывание по уровню, это как ребенок. Если ребенок плачет, вы должны бросить все чем занимались и бежать к ребенку, чтобы покормить его. Потом вы кладете ребенка обратно в кроватку. Если он опять плачет вы от него никуда не отойдете, а будете пытаться успокоить. И пока ребенок плачет вы не будете от него отходить ни на момент, и вернетесь к работе только когда он успокоится. Но предложим, что вышли в сад (прерывание выключено), когда ребенок начал плакать, потом, когда вы вернулись домой (прерывание включено) первое, что вы сделаете пойдете проверить ребенка. Но вы никогда не узнаете, что он плакал, пока вы были в саду.

Прерывание по фронту — это как электронная няня для глухих родителей. Как только ребенок начинает плакать на устройстве загорается красная лампочка и горит, пока вы не нажмете кнопку. Даже если ребенок начал плакать, но быстро перестал и заснул, вы все равно узнаете, что ребенок плакал. Но если он начал плакать, а вы нажали кнопку (подтверждение прерывания), лампочка не будет гореть даже если он продолжает плакать. Уровень звука в комнате должен упасть, а затем подняться снова, чтобы лампочка загорелась.



Небходимо пояснить насчёт EAGAIN и EPOLLET — рекомендация с EAGAIN не относиться к byte-stream, опасность в последнем случае возникает только если вы не вычитали дескриптор до конца, а новые данные не пришли. Тогда в дескрипторе будет висеть "хвост", а нового уведомления вы не получите. С accept() как раз ситуация другая, там вы обязаны продолжать пока accept() не вернет EAGAIN, только в этом случае гарантируется корректная работа.


Проблема громоподобного стада

Представьте огромное количество процессов, ждущих события. Если событие наступает, их всех разбудят и начнётся борьба за ресурсы, хотя требуется только один процесс, который займётся дальнейшей обработкой наступившего события. Остальные процессы снова будут спать.


ас в данном случае интересует проблема распределенных по потокам accept() и read() в связке с epoll.

accept

Собственно, с блокирующимся вызовом accept() никаких проблем давно уже нет. Ядро само позаботится, что только один процесс был разблокирован по данному событию, а все входящие соединения сериализуются.

А вот с epoll такой фокус не пройдет. Если у нас сделан listen() на неблокирующем сокете при установке соединения будут разбужены все epoll_wait() ожидающие событие от данного дескиптора.

Конечно accept() получится сделать только одному потоку, остальные получат EAGAIN, но это напрасная трата ресурсов.

Более того EPOLLET нам так же не поможет, поскольку нам неизвестно сколько именно соединений находится в очереди на подсоединение (backlog). Как мы помним при использовании EPOLLET обработка сокета должна продолжаться до возврата с кодом ошибки EAGAIN, поэтому есть шанс, что все accept() будут обработаны одним потоком, и остальным работы не достанется.

И это опять приводит нас к ситуации, когда соседний поток был разбужен зря.

Так же мы можем получить starvation другого рода — у нас будет загружен только один поток, а остальные не получат соединений для обработки.

EPOLLONESHOT

До версии 4.5 единственным корректным способом обработки распределенного по потокам epoll на неблокирующий listen() дескриптор с послежующим вызовом accept(), было задание флага EPOLLONESHOT, что опять приводило нас к тому, что accept() обрабатывался одновременно только в одном потоке.

В кратце — в случае применения EPOLLONESHOT событие ассоциированное с конкретным дескриптором сработает только один раз, после чего необходимо заново взвести флаги с помощью epoll_ctl().

EPOLLEXCLUSIVE

Здесь нам на помощь приходит EPOLLEXCLUSIVE и level-triggered.

EPOLLEXCLUSIVE разблокирует один ожидающий epoll_wait() за раз на одно событие.

Схема достаточно простая (на самом деле нет):

    У нас N потоков, ожидающих событие на подключение
    К нам подсоединяется первый клиент
    Поток 0 разброкируется и начнет обработку, остальные потоки останутся заблокированными
    К нам подсоединяется второй клиент, если поток 0 все еще занят обработкой, то разблокируется поток 1
    Продолжаем далее пока не исчерпан пул потоков (никто не ожидает события на epoll_wait())
    К нам подсоединяется очередной клиент
    И его обработку получит первый поток, который вызовет epoll_wait()
    Обработку второго клиента получит следующий поток, который вызовет epoll_wait()


Таким образом все обслуживание равномерно распределено по потокам.

https://habr.com/ru/articles/416669/


 Элемент events является набором битов, созданном с помощью следующих возможных типов событий:

EPOLLIN
    Ассоциированный файл доступен для операций read(2). 
EPOLLOUT
    Ассоциированный файл доступен для операций write(2). 
EPOLLPRI
    Нет срочных данных, доступных для операций read(2). 
EPOLLERR
    Произошла ошибка на ассоциированном описателе файлов. 
EPOLLHUP
    На ассоциированном описателе файлов произошло зависание. 
EPOLLET
    Устанавливает поведение Edge Triggered для ассоциированного описателя файлов. Поведением epoll по умолчанию является Level Triggered. Смотрите epoll(4) для более подробной информации об архитектурах событий Edge и Level Triggered. 

Интрерфейс epoll поддерживает все описатели файлов, поддерживаемые poll(2). Корректными значениями для параметра op являются:

EPOLL_CTL_ADD
    Добавить целевой описатель файла fd в описатель epoll - epfd и ассоциировать событие event с внутренним файлом, связанном с fd. 
EPOLL_CTL_MOD
    Изменить событие event связанное с целевым описателем файла fd. 
EPOLL_CTL_DEL
    Убрать целевой описатель файла fd из описателя epoll - epfd. 


*/


/* тест
CheckError(recv(new_tcp_fd, (char *)msg_recv_send, STR_SIZE_MAX, 0),"recv", __LINE__);
 printf("recv блокирующий#получено сообщение TCP > %s\n",msg_recv_send);
 ssize_t size= read(new_tcp_fd,(char *)msg_recv_send,MREAD_SIZE);
  printf("read блокирующий#получено сообщение TCP > %s\n",msg_recv_send);
  */
  
set_nonblocking(udp_fd);
set_nonblocking(new_tcp_fd);
//set_nonblocking(tcp_fd);

int ccount=CLIENTS; //кол-во клиентов


  // create the epoll socket
  int epoll_fd = epoll_create1(0);  //тоже что epol_create только модно поиграться с флагом
                                    // epoll_fd - это тоже файловый дескриптор и его можно засунуть в epoll уровнем повыше к примеру
  if (epoll_fd == -1) {
    perror("epoll_create1()");
    return 1;
  }

struct epoll_event event;
event.events = EPOLLIN | EPOLLHUP | EPOLLET ;   // EPOLLOUT - свободен для записи, EPOLLHUP - клиент разорвал соединение
                                                   //Собственно EPOLLET это то, что делает epoll  мультиплексором для событий.
                                                
 /*   EPOLLET говорит о том что данные готовы к чтению, и не контролирует все данные мы считали или нет из тех что пришли, 
 след событие будет после того как снова что то запишут в дескриптор или закроют его                                        
                               */                 
                                                
event.data.fd=udp_fd;    //мета данные, мог бы быть и указатель - считаем когда наступит событие

if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, udp_fd, &event) == -1) {  // занесем в список слежения
    perror("epoll_ctl()");
    return 1;
  }
  
  event.data.fd=new_tcp_fd;    //мета данные, мог бы быть и указатель - считаем когда наступит событие

if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, new_tcp_fd, &event) == -1) {  // занесем в список слежения
    perror("epoll_ctl()");
    return 1;
  }


while (ccount)
{
 //printf("ждем события\n");
 struct epoll_event events[MAX_EVENTS]; 

  int nevents = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);  // -1 - будем ждать события , 0 выполнить без блокировки
    if (nevents == -1) {
      perror("epoll_wait()");
      return 1;
    }
    //printf("TCP/UDP сервер> Кол-во событий=%d\n",nevents);
    for(int i=0;i<nevents;i++) { // пробежимся по всем событиям
     int fd=events[i].data.fd; //считаем метку на событие, у нас метки - дескрипторы.
     
     if(events[i].events & EPOLLHUP) {
     printf("TCP/UDP сервер>Клиент с дескриптором %d разорвал соединение\n",fd);sleep(5);
     //if(fd==new_tcp_fd) close(new_tcp_fd);
     //if(fd==udp_fd) close(udp_fd);
     close(fd);
     ccount--;
     } else{  // надо читать
             
      
              if(fd==udp_fd){
                             CheckError(recvfrom(udp_fd, (char *)msg_recv_send, STR_SIZE_MAX, 0,(struct sockaddr *)&peer_udp, plen), "recvfrom",__LINE__); 
                             printf("[UDP] получено сообщение>%s<\n",msg_recv_send);
              
                            }
              if(fd==new_tcp_fd){
                             CheckError(recv(new_tcp_fd, (char *)msg_recv_send, STR_SIZE_MAX, 0),"recv", __LINE__);
                             printf("[TCP] получено сообщение>%s<\n",msg_recv_send);
              
                            }
              if((fd!=udp_fd)&&(fd!=new_tcp_fd)){ printf("такого быть не должно!\n");
              
              
                            }

     
           }
     
      
     
    }//посмотрели события

  //CheckError(recv(new_sock_fd, (char *)msg_recv_send, STR_SIZE_MAX, 0),"recv", __LINE__);
 
 
}
  printf("не осталось клиентов, завершаем работу\n");


  CheckError(close(tcp_fd), "close", __LINE__);   // чистим за собой и на выход
  CheckError(close(epoll_fd), "close", __LINE__);


  return 0;
}

/* на select все несколько проще,
1. находим максимальный дескриптор+1, из тех что нужно поставить в слежение, это ускоряет работу selecta, ор перестанет пробегать всю битовую маску по достижении максимального декриптора
 int maxfd=(fd1>fd2) ? fd1 :fd2
 maxfd++;
 
 fd_set readfds;  //флаги
 
 while (1){
 
 FD_ZERO(&readfds); // считаем флаги
 FD_SET(fd1,&readfds); // установим маску
 FD_SET(fd2,&readfds); // установим маску
 
 int redy=select(..)
 if(FD_ISSET(fd1,&readfds) - проверим что на выходе из селекта и выполняем действие
 
 }*/
 
 
 
 
 
 
